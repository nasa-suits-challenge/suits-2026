# FY26 NASA SUITS â€” Athena Voice AIA
**Author:** Matthew Topham  
**Revision:** v3.0 (Context-Aware, Function-Calling Edition)

---

## 1) Overview
Athena is a **context-aware voice AI assistant** for EVA that delivers realâ€‘time, concise guidance and telemetry via a **threeâ€‘tier autonomy hierarchy**:

| Tier | Location | Role |
|---|---|---|
| **1 â€” Earth Megacompute** | Mission Control (high RTT) | Long-horizon analytics, model updates, mission planning |
| **2 â€” HAL Moonbase** | DGX Spark + openai/gptâ€‘ossâ€‘120B | Local highâ€‘performance reasoning, route planning, crew coordination |
| **3 â€” Onâ€‘Suit Core** | Rugged laptop/tablet + passâ€‘through AR | Realâ€‘time STTâ†’Intentâ†’TTS loop, telemetry fusion, checklists, alarms |

Athena is designed to **operate fully offline**; upper tiers only enhance capability.

---

## 2) Objectives
- **Safety & reliability:** bounded, deterministic behavior with explicit guardrails.
- **Low cognitive load:** short, directive speech; HUD cues; modeâ€‘aware prompts.
- **Resilience:** graceful degradation across Earthâ†”HALâ†”Suit links.
- **Testability:** measurable latency/accuracy; clear HITL plan and metrics.

---

## 3) EVA Modes (Finiteâ€‘State Machine)
Phases act as states with bounded intents and tool access.

| Phase | Allowed Intents (examples) | Transition Triggers | Outputs |
|---|---|---|---|
| **Egress** | checklist, vitals, comms check | depress complete â†’ airlock open | HUD checklist; status voice |
| **EVâ€‘Nav** | bearing, breadcrumb, status | rover unlatched â†’ waypoint | nav HUD, course alerts |
| **LTV Repair** | begin/next/back/repeat step, tool check | enter task zone â†’ complete/abort | procedural readouts; verifications |
| **Ingress** | checklist, vitals summary, end EVA | nav return â†’ repress complete | safe return sequence; summary |

---

## 4) Key Requirements & Targets
| Category | Requirement | Target |
|---|---|---|
| **Latency** | Micâ†’spoken reply (deterministic path) | â‰¤ **0.5 s** |
|  | With onâ€‘suit LLM (â‰¤10 tokens) | â‰¤ **0.9 s** |
| **ASR** | Noiseâ€‘robust streaming ASR | **WER < 10%** in field noise |
| **HUD** | Telemetry change â†’ HUD prompt | â‰¤ **300 ms** |
| **Autonomy** | All critical voice flows local | **No loss** under link drop |
| **Guardrails** | Phase whitelist; PTT/confirm for writes | **100%** gated |
| **Logging** | Actuation/event hashes | **100%** recorded |

---

## 5) Hardware Assumptions
- **Onâ€‘Suit Core:** 8â€‘core i7/Ryzen 7, **32 GB RAM** (â‰ˆ16 GB available to models), 1 TB SSD; optional RTX 3050/3060.
- **HMD:** Passâ€‘through AR (Quest Pro / HoloLens 2 / Magic Leap) with mic/speaker.
- **HAL:** DGX Spark (Graceâ€‘Blackwell), **128 GB RAM**, 240 W AC; hosts 120B LLM + planner.
- **Network:** Local Wiâ€‘Fi 7/10 GbE intraâ€‘base (<10 ms oneâ€‘way). SUITSNET provides network/TSS, not compute.

---

## 6) Model Stack (Default)
| Layer | Model | Size | Notes |
|---|---|---:|---|
| VAD | **Silero/WebRTC VAD** | <5 MB | alwaysâ€‘on |
| Wake phrase | **Porcupine** / OpenWakeWord | 20 MB | phraseâ€‘activation + **PTT override** |
| ASR | **Fasterâ€‘Whisper baseâ€‘en (int8)** | ~90 MB | streaming; RNNoise/WebRTC NS |
| Policy/Intent | **Finiteâ€‘state graph** + small LLM | â€” | bounded intents per phase |
| Local LLM | **Llamaâ€‘3.1â€‘8Bâ€‘Instruct (GGUF Q4_K_M)** | ~5.6 GB | short phrasing / intent smoothing |
| TTS | **Piper** (offline) | 60â€“80 MB/voice | preâ€‘baked common lines |
| HAL LLM | **openai/gptâ€‘ossâ€‘120B** | ~100 GB | served via vLLM/TGI; templated directives only |

Alternative onâ€‘suit: Qwenâ€‘2.5â€‘14Bâ€‘Instruct (Q4_K_M ~11 GB) with smaller context (â‰¤1â€“2k).

---

## 7) Context Envelope (Provided Every Turn)
```json
{
  "eva_mode": "EVNAV",
  "procedure": {"id":"LTV-REP-01","step_index":4},
  "telemetry": {"o2_primary_pct":47,"o2_secondary_pct":99,"co2_mmHg":3.8},
  "nav": {"target_marker_id":"m_42","distance_m":85,"bearing_deg":110},
  "alerts": [],
  "comms": {"hal_link":"GREEN","earth_link":"DOWN"},
  "safety": {"ptt_held":false,"glove_mode":true},
  "caps": {"tools_allowed":["get_telemetry","get_checklist_step","repeat_step","get_bearing_to","say"]},
  "policies": {"templates_only":true,"actuation_requires":"PTT_OR_CONFIRM"},
  "time":"2026-05-17T01:14:22Z"
}
```

---

## 8) Toolâ€‘First, Templateâ€‘Only Output
- LLM may call **only** tools in `caps.tools_allowed`; all speech via `say(template_id, slots)`.
- **Every slot** must bind to fields from tool returns/context. No freeâ€‘form prose.
- **Writes** (e.g., `advance_step`, `set_breadcrumb`) require **PTT held** or explicit confirm intent.
- Additional tools (`plan_route`, `summarize_recent`, `rerank_asr`) can be **granted on request** when safe (link GREEN).

---

## 9) KPIs & HITL Testing
- **Latency:** micâ†’speech (deterministic) â‰¤0.5 s; with 8B LLM (â‰¤10 tokens) â‰¤0.9 s.
- **Accuracy:** WER<10% in noise; Intent F1>95%; falseâ€‘wake<1%.
- **Network:** Green/Yellow/Red link emulation; zero critical loss.
- **Usability:** PTT ergonomics with gloves; recovery after link loss.
- **Audit:** Complete event/actuation logs; evidenceâ€‘slot validation 100%.

---

## 10) Deliverables
- Source (voice loop, FSM, tool runtime), **PRD/IMPLEMENTATION**, `tools.json`, `templates.json`.
- HITL test report with plots; demo video of Greenâ†’Redâ†’Green continuity.


## ðŸ§  GPT-OSS-20B â€” Local Suite Model Spec (Recommended Upgrade)

**Model:** `gpt-oss-20b`  
**License:** Apache 2.0 (fully open-source)  
**Size:** 20 billion parameters  
**Memory footprint:** â‰ˆ 15 â€“ 16 GB (fits a 16 GB device with quantization, e.g. Q4_K_M or Q5_K_M)  
**Runtime:** Compatible with `llama.cpp`, `vLLM`, `Ollama`, or OpenAI-compatible API endpoints  
**Context:** up to 32 K tokens  
**Latency (local CPU + GPU hybrid):** ~ 1 s for â‰¤ 10 tokens on a laptop-class GPU; < 700 ms on a desktop RTX 3060/4070  
**Languages:** English-tuned but strong multilingual generalization  
**Quantizations:** GGUF Q4_K_M (â‰ˆ 10 GB), Q5_K_M (â‰ˆ 13 GB), BF16 (â‰ˆ 32 GB full precision)

### ðŸ”§ Native Capabilities
- âœ… **Tool use / Function calling:** OpenAI-style schema (`tools`, `tool_choice`, `tool_calls`)  
- âœ… **Web browsing & retrieval hooks:** designed for plug-in agent runtimes  
- âœ… **Structured JSON / grammar-constrained output**  
- âœ… **Strong short-context reasoning & low hallucination**  
- âœ… **Drop-in OpenAI API compatibility** for local deployments

### ðŸª¶ Recommended EVA Configuration
| Component | Setting |
|------------|----------|
| **Runtime** | `llama.cpp-server` or `Ollama` with OpenAI-compat API |
| **Context** | 2 K â€“ 4 K active (sliding) |
| **Temp / Top-P** | 0.2 â€“ 0.4 / 0.9 |
| **Max new tokens** | â‰¤ 16 |
| **Function call interface** | native `tools` schema |
| **Speech out** | templates only (`say(template_id, slots)`) |
| **Safety** | phase-aware; `ptt_held` or confirm intent required for writes |

### ðŸ›°ï¸ Comparison

| Model | Params | Function Calling | License | Offline Perf (8 core + GPU) | Notes |
|-------|---------|------------------|----------|-------------------------------|-------|
| **GPT-OSS-20B** | 20B | âœ… Native (OpenAI schema) | Apache 2.0 | 400 â€“ 900 ms | Best open function-caller w/ license freedom |
| **Llama 3.1 8B** | 8B | Via grammar/Ollama | Meta license | 300 â€“ 700 ms | Lightweight, deterministic |
| **Qwen 2.5 14B** | 14B | Native (OpenAI schema) | Apache 2.0 | 500 â€“ 1000 ms | Excellent tool agent; more RAM |
| **Gemma 3 12B** | 12B | Partial (prompt based) | Apache 2.0 | 600 â€“ 1100 ms | Great context window; less tool-tuned |

### ðŸš€ Recommended Stack for FY26 SUITS AIA
1. **Local default:** `gpt-oss-20b (Q4_K_M)` â†’ tools & function calls natively.  
2. **Light fallback:** `llama-3.1-8b (Q4_K_M)` â†’ grammar-bound JSON for determinism.  
3. **Optional alternate:** `qwen-2.5-14b (Q4_K_M)` â†’ enhanced tool-planning mode if VRAM allows.  
