# üöÄ Athena Voice AIA ‚Äî FY26 NASA SUITS Proposal

> **Author:** Matthew Topham  
> **Revision:** v3.0 ‚Äî Context-Aware, Function-Calling Edition  
> **Documents:** [`PRD.md`](./PRD.md) ¬∑ [`IMPLEMENTATION.md`](./IMPLEMENTATION.md) ¬∑ [`tools.json`](./tools.json) ¬∑ [`templates.json`](./templates.json)

---

## üß† Overview
**Athena** is a **context-aware voice AI assistant** designed for NASA‚Äôs 2026 **Spacesuit User Interface Technologies for Students (SUITS)** challenge.  
It delivers low-latency, deterministic voice guidance for astronauts through a **three-tier autonomy hierarchy**:

| Tier | Location | Function |
|------|-----------|-----------|
| **1Ô∏è‚É£ Earth Megacompute** | NASA mission control | Long-range planning, model updates |
| **2Ô∏è‚É£ HAL Moonbase** | DGX Spark (openai/gpt-oss-120B) | High-performance reasoning, route planning |
| **3Ô∏è‚É£ On-Suit Core** | Rugged laptop + pass-through AR HMD (openai/gpt-oss-20B) | Real-time STT‚ÜíIntent‚ÜíTTS loop, telemetry fusion, procedural assistance |

All EVA-critical functions run **locally on the suit**, with HAL and Earth tiers providing optional enhancements when network conditions allow.

## üß† GPT-OSS-20B ‚Äî Local Suite Model Spec (Recommended Upgrade)

**Model:** `gpt-oss-20b`  
**License:** Apache 2.0 (fully open-source)  
**Size:** 20 billion parameters  
**Memory footprint:** ‚âà 15 ‚Äì 16 GB (fits a 16 GB device with quantization, e.g. Q4_K_M or Q5_K_M)  
**Runtime:** Compatible with `llama.cpp`, `vLLM`, `Ollama`, or OpenAI-compatible API endpoints  
**Context:** up to 32 K tokens  
**Latency (local CPU + GPU hybrid):** ~ 1 s for ‚â§ 10 tokens on a laptop-class GPU; < 700 ms on a desktop RTX 3060/4070  
**Languages:** English-tuned but strong multilingual generalization  
**Quantizations:** GGUF Q4_K_M (‚âà 10 GB), Q5_K_M (‚âà 13 GB), BF16 (‚âà 32 GB full precision)

### üîß Native Capabilities
- ‚úÖ **Tool use / Function calling:** OpenAI-style schema (`tools`, `tool_choice`, `tool_calls`)  
- ‚úÖ **Web browsing & retrieval hooks:** designed for plug-in agent runtimes  
- ‚úÖ **Structured JSON / grammar-constrained output**  
- ‚úÖ **Strong short-context reasoning & low hallucination**  
- ‚úÖ **Drop-in OpenAI API compatibility** for local deployments

### ü™∂ Recommended EVA Configuration
| Component | Setting |
|------------|----------|
| **Runtime** | `llama.cpp-server` or `Ollama` with OpenAI-compat API |
| **Context** | 2 K ‚Äì 4 K active (sliding) |
| **Temp / Top-P** | 0.2 ‚Äì 0.4 / 0.9 |
| **Max new tokens** | ‚â§ 16 |
| **Function call interface** | native `tools` schema |
| **Speech out** | templates only (`say(template_id, slots)`) |
| **Safety** | phase-aware; `ptt_held` or confirm intent required for writes |

### üõ∞Ô∏è Comparison

| Model | Params | Function Calling | License | Offline Perf (8 core + GPU) | Notes |
|-------|---------|------------------|----------|-------------------------------|-------|
| **GPT-OSS-20B** | 20B | ‚úÖ Native (OpenAI schema) | Apache 2.0 | 400 ‚Äì 900 ms | Best open function-caller w/ license freedom |
| **Llama 3.1 8B** | 8B | Via grammar/Ollama | Meta license | 300 ‚Äì 700 ms | Lightweight, deterministic |
| **Qwen 2.5 14B** | 14B | Native (OpenAI schema) | Apache 2.0 | 500 ‚Äì 1000 ms | Excellent tool agent; more RAM |
| **Gemma 3 12B** | 12B | Partial (prompt based) | Apache 2.0 | 600 ‚Äì 1100 ms | Great context window; less tool-tuned |

---

## ‚öôÔ∏è Core Features
- **Finite-state EVA controller:**  
  Mode-aware behavior across Egress ‚Üí EV-Nav ‚Üí LTV Repair ‚Üí Ingress phases.
- **Function-calling LLM:**  
  Athena calls only verified JSON tools defined in [`tools.json`](./tools.json) to eliminate hallucinations.
- **Template-only speech:**  
  Spoken outputs use deterministic templates from [`templates.json`](./templates.json).
- **Three-tier autonomy:**  
  HAL augments reasoning; suit remains autonomous under comms loss.
- **Green/Yellow/Red link modes:**  
  Ensures graceful degradation and predictable latency.
- **Offline ASR/TTS pipeline:**  
  Faster-Whisper (int8) + Piper (offline VITS) enable < 0.5 s round-trip response.

---

## üß© System Architecture
```
Mic ‚Üí VAD ‚Üí Wake ‚Üí ASR ‚Üí Intent Router (FSM + Local LLM) ‚Üí TTS ‚Üí Speaker/HUD
                          ‚îÇ
                          ‚îú‚îÄ‚îÄ Telemetry (TSS via WebSocket JSON/GeoJSON)
                          ‚îÇ
               HAL (gpt-oss-120B on DGX Spark) ‚Üî Earth (Mission Control)
```

---

## üì° Context Envelope
Every conversational turn includes full EVA state context:
```json
{
  "eva_mode": "EVNAV",
  "procedure": {"id":"LTV-REP-01","step_index":4},
  "telemetry": {"o2_primary_pct":47,"o2_secondary_pct":99},
  "nav": {"distance_m":85,"bearing_deg":110},
  "comms": {"hal_link":"GREEN","earth_link":"DOWN"},
  "caps": {"tools_allowed":["get_telemetry","get_checklist_step","repeat_step","get_bearing_to","say"]}
}
```
The LLM is **context-aware and tool-bound**; it cannot generate free-form text or invoke disallowed tools.

---

## üß∞ Primary Tools (`tools.json`)
Example subset:  
- `get_telemetry(fields[])` ‚Üí suit vitals & environment data  
- `advance_step(procedure_id, from_index)` ‚Üí proceed in checklist *(PTT/confirm required)*  
- `get_bearing_to(marker_id)` ‚Üí direction & distance to breadcrumb  
- `plan_route(from, to, constraints)` ‚Üí optional HAL-granted planner

Each tool includes schema, phase whitelist, and safety requirements.

---

## üó£Ô∏è Templates (`templates.json`)
Example spoken outputs:
- `status_o2`: ‚ÄúPrimary O‚ÇÇ { o2_primary_pct }% ‚Äî Secondary { o2_secondary_pct }%.‚Äù  
- `bearing_simple`: ‚ÄúBearing { bearing_deg } degrees, { distance_m } meters.‚Äù  
- `step_advanced`: ‚ÄúStep { from_index } complete. Proceed to step { to_index }.‚Äù  
Templates ensure every utterance is **grounded in data**.

---

## ‚è±Ô∏è Performance Targets
| Stage | Goal |
|-------|------|
| Mic ‚Üí Speech (deterministic) | ‚â§ 0.5 s |
| With on-suit 8B LLM (‚â§ 10 tokens) | ‚â§ 0.9 s |
| ASR WER in noise | < 10 % |
| HUD refresh from telemetry | ‚â§ 300 ms |
| False-wake rate | < 1 % |

---

## üß© Folder Structure
```
docs/Athena_VoiceAIA_FY26/
 ‚îú‚îÄ‚îÄ PRD.md
 ‚îú‚îÄ‚îÄ IMPLEMENTATION.md
 ‚îú‚îÄ‚îÄ tools.json
 ‚îú‚îÄ‚îÄ templates.json
 
```

---

## üß™ Testing & HITL Plan
- Latency and accuracy benchmarks (mic‚Üíspeech, WER, Intent F1).  
- Network degrade simulation: Green < 10 ms, Yellow 100 ¬± 30 ms (1 % loss), Red = offline.  
- Noise/night trials and glove PTT ergonomics.  
- Slot evidence validation and complete actuation audit trail.

---

## üõ°Ô∏è Safety & Guardrails
- **Mode-bounded command sets** (EVA FSM).  
- **PTT/confirm required** for any state-changing action.  
- **Tool-only reasoning:** LLM outputs strict JSON plans, verified before execution.  
- **Audit logs** include `{ intent, tool, args_hash, result_hash, template_id, eva_mode, ptt_flag }`.

---

## üìö References
- [`PRD.md`](./PRD.md) ‚Äî Product Requirements & Architecture  
- [`IMPLEMENTATION.md`](./IMPLEMENTATION.md) ‚Äî System, models, and HITL plan  
- [`tools.json`](./tools.json) ‚Äî Function-calling surface  
- [`templates.json`](./templates.json) ‚Äî Deterministic speech templates  
- [`runtime/planner_test.py`](../runtime/planner_test.py) ‚Äî Unit tests for planner logic

---

**Athena Voice AIA**  
‚ÄúContext-aware, tool-first intelligence for safe, resilient EVA operations.‚Äù
